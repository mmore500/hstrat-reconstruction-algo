\section{Results and Discussion} \label{sec:results}

\subsection{Runtime Comparison}

In direct comparison between the naive and the shortcut-enabled trie building implementations, we found substantial improvements in execution time under by the latter.
Figure~\ref{fig:comparison} compares execution time for both approaches for problem sizes ranging up to 10,000 genomes.
Over this window, the naive algorithm's runtime appears to grow at a superlinear pace, increase up to 15 minutes for the largest surveyed problem size.
In contrast, runtime for the shortcut-enabled approach at this scale is approximately 3 seconds --- a 300-fold difference in performance.

\input{fig/ntips-comparison.tex}

Although the same programming language was used for both implementations in this trial to ensure apples-to-apples comparability, implementations did differ in the underlying data structure used to store trie data.
The shortcut-enabled implementation used a contiguous edge-list table to store trie data, while the naive implementation used a node-and-pointer approach.
Typically, table-based approaches are faster in practice due to cache locality effects and reduced memory allocation calls.
However, both data structures provide equivalent time complexities for all relevant initialization and lookup operations.
Thus, the notable difference in scaling properties between the shortcut-enabled and naive benchmarks is indicative of meaningful differences in the underlying algorithms, beyond effects attributable to data structure implementations.

\subsection{Microbenchmark: Empirical Scaling Analysis}

\input{fig/scaling.tex}

Having seen a substantial speedup in comparing the shortcut-enabled algorithm to the naive trie-building approach, we next sought to assess the scaling properties of the shortcut-enabled approach in isolation.

Figure~\ref{fig:scaling} depicts the relationship between trie size as 100,000-tip batches are added and the time taken to insert each batch, over 5 replicates of 10 million tip reconstructions.
Fitting a linear regression model to this data, for both the adaptive and purifying regime data sets we found that, with a significance of $\alpha = 0.05$, that we should reject the null hypothesis of a constant slope.
Evidence indicates, therefore, that as trie size increases, the time required per tip added increases as well.

Notably, batch processing times exhibit strong outliers for which trie extension was notably slower.
This effect is due to the presence expensive consolidation steps within those batches.
To assess this phenomenon, we performed a more fine-grained analysis (with batches of 5,000 tips) on non-outlier data (filtering out values above the 95th percentile).
Further regression analysis using this data suggests that past a certain point, non-outlier batch times do not increase significantly with larger tree size.
(See supplemental materials \citep{supplemental} for more information.)
In an analytical investigation of scaling complexity, which is left to future work, we therefore expect the degree to which expensive trie consolidation operations are amortized over average-case insertions to play an important role in determining the scaling complexity of the shortcut-enabled approach.

Microbenchmarks measuring net reconstruction time across problem sizes, rather than marginal batch-wise throughput, reach a similar conclusion and are provided in supplemental Figure~\ref{fig:asymptotic} \citep{supplemental}.

\subsection{Macrobenchmark: One Billion-Tips}

\input{fig/billion-tip-time.tex}

Given the promising results of microbenchmark trials, particularly the favorable empirical scaling result, we next sought to assess the performance of our approach on an extremely large problem size.

For this purpose, we conducted trial reconstruction benchmarks comprising the full corpus of 1 billion agent genomes extracted from our trial Wafer-Scale Engine experiments.
As a point of comparison, the largest reconstructions performed using the previous approach comprised only 10,000 tips \citep{moreno2024trackable}.

% log files:
% https://osf.io/s3rzj and https://osf.io/gj52m
Benchmarked operations included the full reconstruction operation pipeline shown in Figure \ref{fig:hstratschematic}.
Input data comprised a Parquet format file storing raw agent genomes in a hexadecimal string representation.
The population file size was 34 GB for and 35 GB for the sample adaptive- and purifying-selection regimes, respectively.
The output data comprised the reconstructed phylogeny in an edge-list format closely resembling the ALife data standard.
Several pieces of metadata (e.g., agent position and extraction batch) were forwarded to the output data.
For the adaptive-selection regime, the output file size was 54 GB, and for the purifying-selection regime, the output file size was 55 GB.

% log files:
% https://osf.io/s3rzj and https://osf.io/gj52m
Given the large problem-size scale tested, macrobenchmark trials were carried out on a compute cluster node, rather than a desktop PC, as was used for microbenchmark experiments.
Figure \ref{fig:billion-tip-time} profiles net runtime and its breakdown in our case study reconstruction trials.
Reconstruction time was 2h:30m for the adaptive-regime data set.
As expected, given the associated richer phylogenetic history, reconstruction was more intensive for the purifying-regime data set, clocking in at 3h:29m.
These figures correspond to a net throughput of 6.67 and 4.78 million tips per minute, respectively.
Peak memory use for the adaptive and purifying selection regimes was 1.2 TB and 1.1 TB.

\subsection{Validation: Comparison to Ground Truth}

In a first set of trials to validate the quality of phylogenies produced by the shortcut-enabled algorithm, we tested inference accuracy against ground-truth data.
For these trials, we conducted experiments using a neutral evolution model on a single CPU, where we were able to directly record the underlying phylogeny history.
Example visualizations of corresponding reconstruction and ground-truth phylogenies are provided in supplementary materials \citep{supplemental}.

%MAYBE if time permits, it would be ideal to have a second row in this table where we have error measures from the naive algorithm
\input{tab/validation}

Table~\ref{table:validation} reports reconstruction error across annotation sizes ranging from 8 single-bit markers up to 64 single-bit markers.
As expected, medium and large annotation sizes produced lowest reconstruction error, with large annotations of 64 bits achieving triplet distance error on average around 2\%.%
\footnote{This error measure corresponds to the fraction of sampled three-node sets for which both trees report the same two nodes as more closely related.}
In contrast, very small 8-bit annotations averaged 37\% reconstruction error.

%\subsection{Validation: Comparison to Naive Algorithm}

%Given that previous work using the naive reconstruction approach has been extensively validated and analyzed \citep{moreno2025testing}, we additionally assessed reconstruction quality under the shortcut-enabled approach vis-a-vis comparison to the naive approach.

%When comparing trees reconstructed by the shortcut algorithm with those reconstructed by the naive algorithm, we noticed that, for the sub-sample data, the arbitrary choices present in each algorithm made a very large difference when it came to producing different trees.
%Since there is no ground truth phylogeny for this dataset, and that the only way these algorithms can differ is by these arbitrary choices, we can assume that both trees are equally likely to be more accurate.

%However, to try to minimize this difference, we generated synthetic data based on the sample data and attached various stream curation algorithms to discern their effects.
%It was found that algorithms with a higher emphasis on more recent data resulted in less data being present for older generations, leading to more arbitrary choices made in the naive and shortcut algorithms early on, as there is more uncertainty during those generations.
%These choices early on had a large impact on the resulting tree, resulting in the two algorithms having different outputs.
%However, theoretically, both outputs should be equally ``valid'' under the constraints of the naive algorithm.

%Therefore, stream curation protocols with a higher retention of old data resulted in reconstructed trees by the naive and shortcut algorithms being more similar.
%Additionally, we found that trees were more similar by using data where ranks were more similar (from the tail of the billion-tip experiement rather than a sample throughout).
